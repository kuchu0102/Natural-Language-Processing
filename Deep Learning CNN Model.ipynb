{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "lzLwCDTiVjlP",
    "outputId": "41675ff3-3067-455f-b583-6190292b4817"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "import string\n",
    "import scipy.sparse as sparse\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix , log_loss , accuracy_score , classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import recall_score\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import refrom bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rdgxm_dYVjld"
   },
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbB2jWphVjlf"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "jffK1hiLVjlo",
    "outputId": "41705a5c-f742-42c7-c6fe-c94f5d10690f"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6oQKKMUKVjlx",
    "outputId": "a3ff08b2-e80c-4f87-bcfd-304c330c5e04"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl6dGI1VVjl4"
   },
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xvent1LvVjl6"
   },
   "outputs": [],
   "source": [
    "data_nonulls = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "luXpYWTiVjmC",
    "outputId": "4a92771e-1fc6-4966-8058-08c63f9d5055"
   },
   "outputs": [],
   "source": [
    "data_nonulls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "aZaRXw5KVjmI",
    "outputId": "7084d963-ea02-47b7-be51-190257ac755a"
   },
   "outputs": [],
   "source": [
    "data_nonulls['Time'] = pd.to_datetime(data_nonulls['Time'],unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiZZbm56VjmO"
   },
   "source": [
    "# Changing the Score value to category of Positive and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "rjcBWysAVjmP",
    "outputId": "db00174c-fd72-4a58-a527-5321a99fe01c"
   },
   "outputs": [],
   "source": [
    "data_nonulls['Score'] = data[\"Score\"].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripping the non-alphabet words/characters from the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "_-17Cg9fVjmV",
    "outputId": "c88a49d1-b47c-4327-a490-099fcc27fa25"
   },
   "outputs": [],
   "source": [
    "def cleaning_non_alphabet(line):\n",
    "    pattern = re.compile(r'[^a-z]+')\n",
    "    line = line.lower()\n",
    "    line = pattern.sub(' ', line).strip()  \n",
    "    return line\n",
    "data_nonulls['cleaned_Text'] = data_nonulls['Text'].apply(lambda x: cleaning_non_alphabet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "gDn6Bgx3Vjmd",
    "outputId": "3e920300-f156-4968-92ec-6cfda8ed697a"
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(line):\n",
    "    # Tokenize\n",
    "    words = word_tokenize(line)\n",
    "    # stop words\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords_list]\n",
    "    # stemming\n",
    "    ps  = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    # list to sentence\n",
    "    line = ' '.join(words) \n",
    "    return line\n",
    "\n",
    "data_nonulls['cleaned_Text'] = data_nonulls['cleaned_Text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THUXvW8FVjmn"
   },
   "outputs": [],
   "source": [
    "texts = data_nonulls['cleaned_Text']\n",
    "labels = []\n",
    "for idx in data_nonulls['Score']:\n",
    "    labels.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing- Tokenization and sequence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-t1dsbOmVjmu",
    "outputId": "e2b6ca87-82fe-4ef3-b097-db93b451e1b4"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 40000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "KWIVVnO-Vjm0",
    "outputId": "33112e0e-7563-4dca-d1ae-3ee96777e645"
   },
   "outputs": [],
   "source": [
    "#pad_sequences is used to ensure that all sequences in a list have the same length. \n",
    "data = pad_sequences(\n",
    "                    sequences, \n",
    "                    maxlen=MAX_SEQUENCE_LENGTH\n",
    "                    )\n",
    "\n",
    "#to_categorical to convert array of labeled data(from 0 to nb_classes-1) to one-hot vector.\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Data split into Training and Validation\n",
    "\n",
    "split_percentage = 0.05\n",
    "validation_data = int(split_percentage * data.shape[0])\n",
    "\n",
    "x_train = data[:-validation_data]\n",
    "y_train = labels[:-validation_data]\n",
    "x_val = data[-validation_data:]\n",
    "y_val = labels[-validation_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvKnIXt-Vjm5"
   },
   "outputs": [],
   "source": [
    "#Employing Embedding to provide mapping semantic meaning into a geometric space to text data.\n",
    "EMBEDDING_DIM = 200\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CNN model(When One Word Considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#Input Layer\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#Layer 1\n",
    "l_cov1= Conv1D(512, 1,activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(1)(l_cov1)\n",
    "\n",
    "#Layer 2\n",
    "l_cov2 = Conv1D(256, 1, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(1)(l_cov2)\n",
    "\n",
    "#Layer 3\n",
    "l_cov3 = Conv1D(256, 1, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(1)(l_cov3)\n",
    "\n",
    "#Layer 4\n",
    "l_cov4 = Conv1D(128, 1, activation='relu')(l_pool3)\n",
    "l_pool4 = MaxPooling1D(1)(l_cov4)  \n",
    "\n",
    "#Layer 5\n",
    "l_cov5 = Conv1D(128, 1, activation='relu')(l_pool4)\n",
    "l_pool5 = MaxPooling1D(1)(l_cov5)  \n",
    "\n",
    "#Global Flattenning and Dense layer\n",
    "l_flat = Flatten()(l_pool4)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "\n",
    "#Activation Layer\n",
    "preds = Dense(2, \n",
    "              activation='softmax',\n",
    "              kernel_initializer='he_normal')(l_dense)\n",
    "\n",
    "model1 = Model(sequence_input, preds)\n",
    "\n",
    "#Compiling the model\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Displaying the structure of the model\n",
    "model1.summary()\n",
    "\n",
    "#Checkpoint to save the model\n",
    "checkpoint=ModelCheckpoint('cnn_model_1word.hdf5',\n",
    "                   monitor='val_acc',\n",
    "                   verbose=1,\n",
    "                   save_best_only=True\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "\n",
    "history=model1.fit(\n",
    "                 x_train, \n",
    "                 y_train, \n",
    "                 validation_data=(x_val, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size=64,\n",
    "                 callbacks=[checkpoint],\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plotting the Accuracy and Loss results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['acc']\n",
    "validation_accuracy = history.history['val_acc']\n",
    "Train_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "##Plotting the Accuracy for the Model##\n",
    "\n",
    "plt.title('Accuracy Plot')\n",
    "plt.plot(epochs,accuracy, 'green', label='Training Accuracy')\n",
    "plt.plot(epochs,validation_accuracy, 'brown', label='Test Accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True, fontsize='small')\n",
    "legend.get_frame().set_facecolor('C0')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "def annot_max(x,y, ax=None):\n",
    "    xmax = x[np.argmax(y)]\n",
    "    ymax = max(y)\n",
    "    text= \"Maximum Accuracy={:.5f}\".format(ymax*100)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.50)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=90\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymax), xytext=(0.60,0.60), **kw)\n",
    "\n",
    "annot_max(epochs,validation_accuracy)\n",
    "plt.grid(True)\n",
    "plt.savefig('2wordsAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "##Plotting the Loss for the Model##\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Loss Plot')\n",
    "plt.plot(epochs,Train_loss, 'green', label='Training loss')\n",
    "plt.plot(epochs,validation_loss, 'brown', label='Test loss')\n",
    "legend = plt.legend(loc='best', shadow=True, fontsize='small')\n",
    "legend.get_frame().set_facecolor('C0')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "def annot_min(x,y, ax=None):\n",
    "    xmax = x[np.argmin(y)]\n",
    "    ymin = min(y)\n",
    "    text= \"Minimum loss={:.5f}\".format(ymin)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.50)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=90\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymin), xytext=(0.90,0.90), **kw)\n",
    "\n",
    "annot_min(epochs,validation_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.savefig('2wordsLoss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CNN model(When Two Words Considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "colab_type": "code",
    "id": "YsoChEe1VjnG",
    "outputId": "685106d2-07b0-4580-b5db-9c5501a385f2"
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "#Input Layer\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "#Layer 1\n",
    "l_cov1= Conv1D(512, 1,activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(pool_size=2, strides=1)(l_cov1)\n",
    "\n",
    "#Layer 2\n",
    "l_cov2 = Conv1D(256, 1, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(pool_size=2, strides=1)(l_cov2)\n",
    "\n",
    "#Layer 3\n",
    "l_cov3 = Conv1D(256, 1, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(pool_size=2, strides=1)(l_cov3)\n",
    "\n",
    "#layer 4\n",
    "l_cov4 = Conv1D(128, 1, activation='relu')(l_pool3)\n",
    "l_pool4 = MaxPooling1D(pool_size=2, strides=1)(l_cov4)  \n",
    "\n",
    "#Model flattening and dense layers\n",
    "l_flat = Flatten()(l_pool4)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "\n",
    "#Applying Activation\n",
    "preds = Dense(2, activation='softmax',kernel_initializer='he_normal')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "#Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Displaying the structure of the model\n",
    "model.summary()\n",
    "\n",
    "#Checkpoint to save the model\n",
    "checkpoint2=ModelCheckpoint(\n",
    "                    'model_cnn.hdf5.hdf5',\n",
    "                    monitor='val_acc',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True\n",
    "                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "k3bf43cnVjng",
    "outputId": "1e6969fa-6acf-4330-9c67-ce354687403d"
   },
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "\n",
    "history1=model.fit(\n",
    "                 x_train, \n",
    "                 y_train, \n",
    "                 validation_data=(x_val, y_val),\n",
    "                 epochs=10, \n",
    "                 batch_size=64,\n",
    "                 callbacks=[checkpoint2],\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plotting the Accuracy and Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koxeKvrLVjnr"
   },
   "outputs": [],
   "source": [
    "accuracy = history1.history['acc']\n",
    "validation_accuracy = history1.history['val_acc']\n",
    "Train_loss = history1.history['loss']\n",
    "validation_loss = history1.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "##Plotting the Accuracy for the Model##\n",
    "\n",
    "plt.title('Accuracy Plot')\n",
    "plt.plot(epochs,accuracy, 'green', label='Training Accuracy')\n",
    "plt.plot(epochs,validation_accuracy, 'brown', label='Test Accuracy')\n",
    "legend = plt.legend(loc='best', shadow=True, fontsize='small')\n",
    "legend.get_frame().set_facecolor('C0')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "def annot_max(x,y, ax=None):\n",
    "    xmax = x[np.argmax(y)]\n",
    "    ymax = max(y)\n",
    "    text= \"Maximum Accuracy={:.5f}\".format(ymax*100)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.50)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=90\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymax), xytext=(0.60,0.60), **kw)\n",
    "\n",
    "annot_max(epochs,validation_accuracy)\n",
    "plt.grid(True)\n",
    "plt.savefig('2wordsAccuracy.png')\n",
    "plt.show()\n",
    "\n",
    "##Plotting the Loss for the Model##\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Loss Plot')\n",
    "plt.plot(epochs,Train_loss, 'green', label='Training loss')\n",
    "plt.plot(epochs,validation_loss, 'brown', label='Test loss')\n",
    "legend = plt.legend(loc='best', shadow=True, fontsize='small')\n",
    "legend.get_frame().set_facecolor('C0')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "def annot_min(x,y, ax=None):\n",
    "    xmax = x[np.argmin(y)]\n",
    "    ymin = min(y)\n",
    "    text= \"Minimum loss={:.5f}\".format(ymin)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.50)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=90\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymin), xytext=(0.90,0.90), **kw)\n",
    "\n",
    "annot_min(epochs,validation_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.savefig('2wordsLoss.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project -cnn 2grams.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
